{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CSE 5095: Social Media Mining and Analysis**\n",
    "Fall 2024, Assignment #1, 200 points\n",
    "\n",
    "\n",
    "In this assignment, we will explore the statistical properties of the quantitative features associated with each subreddit in your data set. Each data set has observations from two subreddits. In some data sets, each observation is a post, whereas for the other data sets each observation is a compilation of comments for each unique post.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 1: Descriptive Statistics (50 points)**\n",
    "Build a table with the descriptive statistics (mean and variance) of the quantitative features for each\n",
    "subreddit in your data set. For projects based on post-level data, these will include the post-level statistics\n",
    "shown in Table 1. For projects based on comment data, these include the average of average commentlevel statistics as shown in Table 2 (average comment and user statistics are computed for each post). For\n",
    "comment data, include also user-level features listed in Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mso21001\\AppData\\Local\\Temp\\ipykernel_16904\\2429931876.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  stats = df.groupby('subreddit').apply(calc_stats).reset_index()\n"
     ]
    }
   ],
   "source": [
    "def calc_stats(group):\n",
    "    return pd.DataFrame({\n",
    "        'mean': group[features].mean(),\n",
    "        'variance': group[features].var()\n",
    "    })\n",
    "\n",
    "\n",
    "df = pd.read_csv('project10.csv')\n",
    "\n",
    "post_features = ['post_score', 'post_upvote_ratio', 'post_thumbs_ups', 'post_total_awards_received']\n",
    "comment_features = ['score', 'controversiality', 'ups', 'downs']\n",
    "user_features = ['user_awardee_karma', 'user_awarder_karma', 'user_link_karma', 'user_comment_karma', 'user_total_karma']\n",
    "\n",
    "is_post_level = all(feature in df.columns for feature in post_features)\n",
    "if is_post_level:\n",
    "    features = post_features + user_features\n",
    "else:\n",
    "    features = comment_features + user_features\n",
    "\n",
    "stats = df.groupby('subreddit').apply(calc_stats).reset_index()\n",
    "stats = stats.pivot(index='level_1', columns='subreddit', values=['mean', 'variance'])\n",
    "\n",
    "subreddits = stats.columns.get_level_values(1).unique()\n",
    "new_order = [(stat, subreddit) for subreddit in subreddits for stat in ['mean', 'variance']]\n",
    "stats = stats.reindex(columns=new_order)\n",
    "\n",
    "stats = stats.round(2)\n",
    "stats.index.name = 'Feature'\n",
    "stats.columns.names = ['Statistic', 'Subreddit']\n",
    "\n",
    "stats.to_csv('subreddit_statistics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 2: Distributions (50 points)**\n",
    "\n",
    "For each quantitative feature, plot the two distributions corresponding to the two subreddits. Comment on\n",
    "the properties of each distribution (symmetrical, left-skewed, right-skewed), and how they compare with\n",
    "each other. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "science - post_score:\n",
      "  Mean: 180.31\n",
      "  Median: 60.00\n",
      "  Distribution: right-skewed\n",
      "  Range: 0.00 to 4951.00\n",
      "\n",
      "action - post_score:\n",
      "  Mean: 94.45\n",
      "  Median: 41.00\n",
      "  Distribution: right-skewed\n",
      "  Range: 0.00 to 880.00\n",
      "\n",
      "science - post_upvote_ratio:\n",
      "  Mean: 0.93\n",
      "  Median: 0.96\n",
      "  Distribution: left-skewed\n",
      "  Range: 0.04 to 1.00\n",
      "\n",
      "action - post_upvote_ratio:\n",
      "  Mean: 0.93\n",
      "  Median: 0.97\n",
      "  Distribution: left-skewed\n",
      "  Range: 0.12 to 1.00\n",
      "\n",
      "science - post_thumbs_ups:\n",
      "  Mean: 180.31\n",
      "  Median: 60.00\n",
      "  Distribution: right-skewed\n",
      "  Range: 0.00 to 4951.00\n",
      "\n",
      "action - post_thumbs_ups:\n",
      "  Mean: 94.45\n",
      "  Median: 41.00\n",
      "  Distribution: right-skewed\n",
      "  Range: 0.00 to 880.00\n",
      "\n",
      "science - post_total_awards_received:\n",
      "  Mean: 0.00\n",
      "  Median: 0.00\n",
      "  Distribution: right-skewed\n",
      "  Range: 0.00 to 0.00\n",
      "\n",
      "action - post_total_awards_received:\n",
      "  Mean: 0.00\n",
      "  Median: 0.00\n",
      "  Distribution: right-skewed\n",
      "  Range: 0.00 to 0.00\n",
      "\n",
      "science - user_awardee_karma:\n",
      "  Mean: 1944.27\n",
      "  Median: 127.00\n",
      "  Distribution: right-skewed\n",
      "  Range: 0.00 to 154464.00\n",
      "\n",
      "action - user_awardee_karma:\n",
      "  Mean: 2089.47\n",
      "  Median: 76.00\n",
      "  Distribution: right-skewed\n",
      "  Range: 0.00 to 36848.00\n",
      "\n",
      "science - user_awarder_karma:\n",
      "  Mean: 605.58\n",
      "  Median: 0.00\n",
      "  Distribution: right-skewed\n",
      "  Range: 0.00 to 43075.00\n",
      "\n",
      "action - user_awarder_karma:\n",
      "  Mean: 575.29\n",
      "  Median: 0.00\n",
      "  Distribution: right-skewed\n",
      "  Range: 0.00 to 53073.00\n",
      "\n",
      "science - user_link_karma:\n",
      "  Mean: 90178.09\n",
      "  Median: 1058.00\n",
      "  Distribution: right-skewed\n",
      "  Range: 0.00 to 11490136.00\n",
      "\n",
      "action - user_link_karma:\n",
      "  Mean: 47975.48\n",
      "  Median: 878.00\n",
      "  Distribution: right-skewed\n",
      "  Range: 0.00 to 3141592.00\n",
      "\n",
      "science - user_comment_karma:\n",
      "  Mean: 66500.80\n",
      "  Median: 22298.00\n",
      "  Distribution: right-skewed\n",
      "  Range: -100.00 to 1015526.00\n",
      "\n",
      "action - user_comment_karma:\n",
      "  Mean: 77849.73\n",
      "  Median: 7851.00\n",
      "  Distribution: right-skewed\n",
      "  Range: -100.00 to 3640897.00\n",
      "\n",
      "science - user_total_karma:\n",
      "  Mean: 159228.75\n",
      "  Median: 28091.00\n",
      "  Distribution: right-skewed\n",
      "  Range: -99.00 to 11990059.00\n",
      "\n",
      "action - user_total_karma:\n",
      "  Mean: 128489.97\n",
      "  Median: 11608.00\n",
      "  Distribution: right-skewed\n",
      "  Range: -80.00 to 6287054.00\n"
     ]
    }
   ],
   "source": [
    "def is_skewed(data):\n",
    "    return abs(stats.skew(data)) > 1\n",
    "\n",
    "# Function to determine skewness\n",
    "def get_skewness(data):\n",
    "    skewness = skew(data)\n",
    "    if abs(skewness) < 0.5:\n",
    "        return \"approximately symmetrical\"\n",
    "    elif skewness < 0:\n",
    "        return \"left-skewed\"\n",
    "    else:\n",
    "        return \"right-skewed\"\n",
    "\n",
    "# Function to plot distributions and print comments\n",
    "def plot_and_comment(feature):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for subreddit in df['subreddit'].unique():\n",
    "        data = df[df['subreddit'] == subreddit][feature]\n",
    "        sns.histplot(data, kde=True, label=subreddit)\n",
    "        \n",
    "        # Calculate and print statistics\n",
    "        mean = data.mean()\n",
    "        median = data.median()\n",
    "        skewness = get_skewness(data)\n",
    "        print(f\"\\n{subreddit} - {feature}:\")\n",
    "        print(f\"  Mean: {mean:.2f}\")\n",
    "        print(f\"  Median: {median:.2f}\")\n",
    "        print(f\"  Distribution: {skewness}\")\n",
    "        print(f\"  Range: {data.min():.2f} to {data.max():.2f}\")\n",
    "        \n",
    "    plt.title(f'Distribution of {feature} by Subreddit')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{feature}_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "# Plot distributions for each feature\n",
    "for feature in features:\n",
    "    plot_and_comment(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 3: Statistical Significance (25 points)**\n",
    "For each quantitative feature from task 1, assess the statistical significance (at 5% level) among the two\n",
    "subreddits. Refer to the distributions of each feature in Task 2 to determine which statistical test would be\n",
    "the most appropriate, for example, if the data follows a near-symmetric distribution them the t-test might be\n",
    "the most appropriate. On the other hand, if the data follows a highly skewed distribution, then a nonparametric test will be appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Test  Statistic       P-value Significant at 5% level\n",
      "Feature                                                                                         \n",
      "post_score                  Mann-Whitney U test  1366425.0  3.433923e-16                     Yes\n",
      "post_upvote_ratio           Mann-Whitney U test  1068302.0  2.153275e-04                     Yes\n",
      "post_thumbs_ups             Mann-Whitney U test  1366425.0  3.433923e-16                     Yes\n",
      "post_total_awards_received               T-test        NaN           NaN                      No\n",
      "user_awardee_karma          Mann-Whitney U test  1248807.5  4.322622e-04                     Yes\n",
      "user_awarder_karma          Mann-Whitney U test  1220697.0  8.009958e-03                     Yes\n",
      "user_link_karma             Mann-Whitney U test  1201743.0  1.060519e-01                      No\n",
      "user_comment_karma          Mann-Whitney U test  1454788.0  1.859565e-31                     Yes\n",
      "user_total_karma            Mann-Whitney U test  1422186.0  3.304192e-25                     Yes\n",
      "\n",
      "Results have been saved to 'statistical_test_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# Function to determine skewness\n",
    "def is_skewed(data):\n",
    "    return abs(data.skew()) > 1\n",
    "\n",
    "# Function to perform statistical test\n",
    "def perform_test(feature):\n",
    "    subreddit1, subreddit2 = df['subreddit'].unique()\n",
    "    data1 = df[df['subreddit'] == subreddit1][feature]\n",
    "    data2 = df[df['subreddit'] == subreddit2][feature]\n",
    "    \n",
    "    # Check for skewness\n",
    "    if is_skewed(data1) or is_skewed(data2):\n",
    "        # If skewed, use Mann-Whitney U test\n",
    "        statistic, p_value = mannwhitneyu(data1, data2)\n",
    "        test_name = \"Mann-Whitney U test\"\n",
    "    else:\n",
    "        # If not skewed, use t-test\n",
    "        statistic, p_value = ttest_ind(data1, data2)\n",
    "        test_name = \"T-test\"\n",
    "    \n",
    "    return test_name, statistic, p_value\n",
    "\n",
    "# Perform tests for each feature\n",
    "results = []\n",
    "for feature in features:\n",
    "    test_name, statistic, p_value = perform_test(feature)\n",
    "    significant = \"Yes\" if p_value < 0.05 else \"No\"\n",
    "    results.append({\n",
    "        'Feature': feature,\n",
    "        'Test': test_name,\n",
    "        'Statistic': statistic,\n",
    "        'P-value': p_value,\n",
    "        'Significant at 5% level': significant\n",
    "    })\n",
    "\n",
    "# Create and display results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.set_index('Feature')\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('statistical_test_results.csv')\n",
    "print(\"\\nResults have been saved to 'statistical_test_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_features = results_df[results_df['Significant at 5% level'] == 'Yes'].index\n",
    "\n",
    "for feature in significant_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=df, x=feature, hue='subreddit', kde=True, element=\"step\")\n",
    "    plt.title(f'Distribution of {feature} by Subreddit')\n",
    "    plt.savefig(f'{feature}_distribution.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Absolute Difference  Relative Difference\n",
      "Feature                                                          \n",
      "hour_entropy                        0.096639             0.032373\n",
      "peak_day                            5.000000             1.428571\n",
      "day_entropy                         0.003618             0.001885\n",
      "peak_month                          0.000000             0.000000\n",
      "month_entropy                       0.502769             0.227961\n",
      "mean_time_between_posts            19.393762             1.632588\n",
      "std_time_between_posts             24.078345             1.591799\n",
      "posts_per_day                       9.889222             1.632416\n",
      "burstiness                          0.057072             0.398505\n",
      "\n",
      "Results have been saved to 'time_feature_differences.csv'\n",
      "\n",
      "Heatmap of aggregated time features has been saved as 'time_features_heatmap.png'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mso21001\\AppData\\Local\\Temp\\ipykernel_16904\\749338394.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  aggregated_features = df.groupby('subreddit').apply(compute_aggregated_time_features).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('project10.csv')\n",
    "\n",
    "# Convert post_created_time to datetime\n",
    "df['post_created_time'] = pd.to_datetime(df['post_created_time'])\n",
    "\n",
    "# Feature Engineering\n",
    "def compute_aggregated_time_features(df):\n",
    "    features = {}\n",
    "    \n",
    "    # 1. Hour of day distribution\n",
    "    hour_dist = df['post_created_time'].dt.hour.value_counts(normalize=True).sort_index()\n",
    "    features['peak_hour'] = hour_dist.idxmax()\n",
    "    features['hour_entropy'] = stats.entropy(hour_dist)\n",
    "    \n",
    "    # 2. Day of week distribution\n",
    "    dow_dist = df['post_created_time'].dt.dayofweek.value_counts(normalize=True).sort_index()\n",
    "    features['peak_day'] = dow_dist.idxmax()\n",
    "    features['day_entropy'] = stats.entropy(dow_dist)\n",
    "    \n",
    "    # 3. Month distribution\n",
    "    month_dist = df['post_created_time'].dt.month.value_counts(normalize=True).sort_index()\n",
    "    features['peak_month'] = month_dist.idxmax()\n",
    "    features['month_entropy'] = stats.entropy(month_dist)\n",
    "    \n",
    "    # 4. Posting regularity\n",
    "    time_diffs = df['post_created_time'].sort_values().diff().dt.total_seconds() / 3600  # in hours\n",
    "    features['mean_time_between_posts'] = time_diffs.mean()\n",
    "    features['std_time_between_posts'] = time_diffs.std()\n",
    "    \n",
    "    # 5. Temporal density\n",
    "    time_range = (df['post_created_time'].max() - df['post_created_time'].min()).total_seconds() / 3600 / 24  # in days\n",
    "    features['posts_per_day'] = len(df) / time_range if time_range > 0 else 0\n",
    "    \n",
    "    # 6. Burstiness\n",
    "    if len(time_diffs) > 1:\n",
    "        features['burstiness'] = (time_diffs.std() - time_diffs.mean()) / (time_diffs.std() + time_diffs.mean())\n",
    "    else:\n",
    "        features['burstiness'] = 0\n",
    "    \n",
    "    return pd.Series(features)\n",
    "\n",
    "# Compute features for each subreddit\n",
    "aggregated_features = df.groupby('subreddit').apply(compute_aggregated_time_features).reset_index()\n",
    "\n",
    "# Function to test statistical significance\n",
    "def test_significance(feature):\n",
    "    data1 = aggregated_features.loc[aggregated_features['subreddit'] == aggregated_features['subreddit'].iloc[0], feature]\n",
    "    data2 = aggregated_features.loc[aggregated_features['subreddit'] == aggregated_features['subreddit'].iloc[1], feature]\n",
    "    \n",
    "    # Since we have only one value per subreddit, we can't perform statistical tests\n",
    "    # Instead, we'll calculate the absolute difference and relative difference\n",
    "    abs_diff = abs(data1.iloc[0] - data2.iloc[0])\n",
    "    rel_diff = abs_diff / ((data1.iloc[0] + data2.iloc[0]) / 2) if (data1.iloc[0] + data2.iloc[0]) != 0 else 0\n",
    "    \n",
    "    return abs_diff, rel_diff\n",
    "\n",
    "# Test significance for each new feature\n",
    "results = []\n",
    "\n",
    "for feature in aggregated_features.columns[2:]:  # Skip 'subreddit' and 'level_1' columns\n",
    "    abs_diff, rel_diff = test_significance(feature)\n",
    "    results.append({\n",
    "        'Feature': feature,\n",
    "        'Absolute Difference': abs_diff,\n",
    "        'Relative Difference': rel_diff\n",
    "    })\n",
    "\n",
    "# Create and display results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.set_index('Feature')\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('time_feature_differences.csv')\n",
    "print(\"\\nResults have been saved to 'time_feature_differences.csv'\")\n",
    "\n",
    "# Visualize features\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(aggregated_features.set_index('subreddit').iloc[:, 1:], annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Aggregated Time Features by Subreddit')\n",
    "plt.tight_layout()\n",
    "plt.savefig('time_features_heatmap.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nHeatmap of aggregated time features has been saved as 'time_features_heatmap.png'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
      "\n",
      "Jupyter command `jupyter-nbconvert` not found.\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert my-notebook.ipynb --to python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
